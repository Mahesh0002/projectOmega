# OMEGA-PROTOCOL: Neuro-Symbolic Engine for Chaotic Dynamics Discovery

> **"The search for the solution is not random; it is gravitational. We are not wandering in the dark; we are being pulled towards a global optimum."**

The **OMEGA Protocol** is a high-performance, neuro-symbolic symbolic regression suite. Unlike standard deep learning architectures that approximate patterns within a latent space, OMEGA utilizes an autonomous "Teacher-Student" framework to reverse-engineer the exact, mathematically closed-form identities governing a system from raw, noisy observational data.

---

## I. Core Philosophy: The Search Dynamics

The discovery process operates on a two-phase optimization loop:

- **Divergence Phase (Exploration):** Initial search trajectories must be strictly **orthogonal** (perpendicular in vector space) to maximize coverage of the symbolic search space and avoid local minima.
- **Convergence Phase (Exploitation):** Sub-components and successful residual blocks must **merge**, utilizing global refinement to resolve all paths toward the singular true physical law.

---

## II. System Architecture

The engine functions through the asynchronous interaction of three primary logical modules:

### 1. The Explorer (Generative Policy)

A recurrent neural network (LSTM/Transformer) that performs rapid, heuristic traversal of the discrete symbolic search space. It operates strictly forward, generating topological skeletons of mathematical equations without computing constant values.

### 2. The Alchemist (Continuous Optimizer)

A heavy-compute manager utilizing the **L-BFGS** algorithm with Strong-Wolfe line search. It optimizes the continuous physical constants within the discrete skeletons generated by the Explorer. It also recycles discarded symbolic branches, stripping them for sub-expressions to synthesize new hybrid structures.

### 3. The Shadow (Intervention Manager)

A parallel surveillance node. If the primary exploration path stagnates (as measured by validation loss plateaus), the Shadow node injects structural diversity by retrieving heavily mutated or archived structures from the elite registry to force the system out of local minima.

---

## III. Engine Constraints

Every discovery module within the OMEGA suite is strictly governed by the following architectural constraints:

1. **Experience Replay (Conservation):** No generated structure is entirely discarded. Failed architectures are sent to an offline buffer for potential genetic hybridization.
2. **Path Orthogonality:** Search redundancy is strictly prohibited. If two candidate predictions correlate at $>0.95$, the structurally heavier model is immediately pruned.
3. **Parameter Inheritance:** Child structures generated via mutation inherit the optimized continuous constants and momentum states of their parent topologies.
4. **Decoupled Search:** The topological structure is strictly the domain of the Explorer; parameter precision is strictly the domain of the Alchemist.
5. **Differentiable Execution (The Iron Dome):** The computational graph must not break. Singularities (e.g., division by zero) are managed via soft-guard epsilon routing and identity mapping to maintain gradient flow during backpropagation.

---

## IV. Evaluation Environments (The Benchmarks)

The engine is validated against a gauntlet of specific mathematical and physical failure points:

### Phase 1: Asymptotic & Frequency Limits

- **Singularity Benchmark:** Targets $y = 1/(t-2.5)$. The agent must construct algebraic division and survive the vertical asymptote without gradient explosion.
- **Exponential Oscillation (Nightmare):** Targets $y = \sin(e^{1.7t})$. The agent must identify a frequency acceleration that increases exponentially, utilizing a custom "Inlier Maximization" objective.
- **The Titan Benchmark:** Targets $y = \tan(t) + \sin(e^t)$. A highly non-linear hybrid forcing the model to resolve both repeating singularities and chaotic frequencies simultaneously using a multi-island search.

### Phase 2: Chaotic Dynamics

- **Target:** The Lorenz System.
- **Objective:** Reconstruct the three coupled, non-linear differential equations ($\frac{dx}{dt}, \frac{dy}{dt}, \frac{dz}{dt}$) governing chaotic convection purely from noisy time-series trajectories.

### Phase 3: Multi-Body Physics

- **Target:** N-Body Orbital Mechanics.
- **Objective:** Infer the Inverse Square Law ($F = G \frac{m_1 m_2}{r^2}$) and isolate secondary planetary perturbations using Iterative Residual Fitting (Protocol Zero).

---

## V. Optimization Mechanisms

OMEGA employs several advanced heuristics to accelerate convergence:

- **Early Stopping (The Razor):** Greedy pruning that terminates mathematically unviable structures within the first 5% of the data evaluation step.
- **Semantic Caching (Akashic Record):** A global memoization hash-map that stores the discrete output signatures of equations. Retrieval cost is $O(1)$, preventing redundant L-BFGS passes.
- **Curriculum Learning:** The time-series data is unrolled progressively. The model must satisfy error thresholds on an "Easy Window" (e.g., $t=0$ to $t=1$) before unlocking the full operational domain.
- **Jittered Evaluation (Earthquake Test):** Validates structural integrity by injecting Gaussian noise into the time-domain during evaluation. This ensures the engine has discovered a generalized physical law rather than an overfit polynomial curve.

---

## VI. Operation Lifecycle

| Step  | Phase        | Execution Details                                                     |
| :---- | :----------- | :-------------------------------------------------------------------- |
| **1** | **Diverge**  | Explorer generates orthogonal mathematical skeletons.                 |
| **2** | **Filter**   | Semantic Cache eliminates structural and functional duplicates.       |
| **3** | **Prune**    | Early Stopping removes mathematically unstable or weak topologies.    |
| **4** | **Tune**     | Alchemist optimizes leaf-tensor constants via L-BFGS.                 |
| **5** | **Recycle**  | Sub-optimal expressions are broken down for structural hybridization. |
| **6** | **Converge** | Global Refinement (Wet-on-Wet optimization) harmonizes all layers.    |

---

## VII. Execution

**Prerequisites:** Python 3.8+, PyTorch, NumPy, SymPy, Matplotlib.

**Running a Benchmark:**
All specialized benchmarks import the core architecture from `src/omega_industrial.py`. To execute the multi-body planetary physics extraction:

```bash
python benchmarks/planetary_movements.py
```
